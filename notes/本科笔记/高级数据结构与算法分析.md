# ADS期末复习

## 1. 高级数据结构

### 1.1 Tree

1. AVL Tree

   目标：用二叉搜索树提高搜索速度

   定义：左右子树的高度之差不大于1， 所有节点的BF都是-1，0，1

   BF（balance factor）：左右子树的高度差

   四种操作手段：

   * RR rotation（single rotation）

     ![截屏2022-05-29 上午11.13.42](https://whm-image.oss-cn-beijing.aliyuncs.com/img/202206261343301.png)

   * LL rotation（single rotation）

     ![截屏2022-05-29 上午11.13.55](https://whm-image.oss-cn-beijing.aliyuncs.com/img/202206261343654.png)

   * LR rotation（double rotation）

     ![截屏2022-05-29 上午11.14.08](https://whm-image.oss-cn-beijing.aliyuncs.com/img/202206261343750.png)

   * RL rotation（double rotation）

     ![截屏2022-05-29 上午11.14.22](https://whm-image.oss-cn-beijing.aliyuncs.com/img/202206261343240.png)

   > tips：
   >
   > 一些节点的插入可能不会引起树的旋转，只会改变BF的值
   >
   > 假设有N个节点，AVL数的高度是$O(logN)$的
   >
   > 对于高度为k的AVL树（设空树的高度为0），则它的最小节点个数为：
   >
   > $
   > n_k = n_{k-1} + n_{k-2} + 1 \\ n_k = F_{k+2} - 1 \\ k = O(ln(n))
   > $

2. Splay Tree

   目标：从空树开始，任何M次连续的操作最多消耗O（MlogN）的时间

   实现思路：在任意一个节点被访问后，将它移动到AVL树的根节点

   实现方法：根据其父节点和祖父节点

   * 父节点就是root：将该节点与父节点进行single rotation

   * 父节点不是root：

     * zig-zag：double rotation(LR or RL)

       ![截屏2022-05-29 上午11.16.37](https://whm-image.oss-cn-beijing.aliyuncs.com/img/202206261343852.png)

     * zig-zig：two single rotation(LL or RR)

       ![截屏2022-05-29 上午11.16.50](https://whm-image.oss-cn-beijing.aliyuncs.com/img/202206261343782.png)

       

   特点：splaying不仅实现了将被访问节点置于根节点，还实现了将该节点所在路径上的节点的深度减半的作用

   删除节点：

   * 查找该节点，使该节点位于根部
   * 移走该节点
   * 查找左子树的最大值：使左子树的最大值称为左子树的根节点，此时根节点只有左子树
   * 把右子树接到左子树上

3. Red-Black Tree

   目的：构造另一种平衡二叉树

   特点：

   * 每个节点都是红色或黑色
   * 根节点是黑色
   * 叶节点是黑色（叶节点是NULL把它也视为黑色的节点）
   * 如果一个节点是红色，它的子节点必须是黑色
   * 任何一个节点左右两个子树黑色节点数目相同

   ![截屏2022-05-29 上午11.30.35](https://whm-image.oss-cn-beijing.aliyuncs.com/img/202206261343393.png)

   bh(black-height): node x到叶节点路径上的黑色节点的个数（不包括x），计作bh(x), bh(Tree) = bh(root)，性质：

   * For any node x, sizeof(x) >= 2bh(x) – 1.
   * ⼀个有N个internal node的红⿊树的⾼度⼩于等于2ln(N+1)

   Insert 插入操作：

   * 先将插入的节点设置为红色，如果父节点为黑色则保持，若为红色则需要调整

   ![截屏2022-05-29 上午11.36.50](https://whm-image.oss-cn-beijing.aliyuncs.com/img/202206261343358.png)

   Delete 删除操作：

   * 删除叶节点：如果是红节点则将它改为NIL，黑节点还需要再做调整
   * 删除只有一个子节点的节点：将它与它的子节点交换，但是不改变位置与颜色的相对关系
   * 删除有两个子节点的节点：
     * 用左子树最大节点或右子树的最小节点将它替换
     * 删除替换后的节点

   处理黑色叶节点的情况：

   ![截屏2022-05-29 上午11.40.54](https://whm-image.oss-cn-beijing.aliyuncs.com/img/202206261343580.png)

   Number of rotations:

   |           | AVL     | Red-Black Tree |
   | --------- | ------- | -------------- |
   | Insertion | <= 2    | <= 2           |
   | Deletion  | O(logN) | <= 3           |

4. B+ Tree

   定义：有以下特点的M阶树

   * 根节点要么是叶节点，要么有2～M个子节点
   * 除了根节点的所有非叶节点都有M / 2（上取整）- M个叶节点
   * 所有叶节点的深度相同
   * 每个叶节点都有M / 2（上取整）- M个值
   * B+ tree of order 4 也被称为2-3-4树， order3的被称为2-3 tree

   ![截屏2022-05-29 上午11.48.34](https://whm-image.oss-cn-beijing.aliyuncs.com/img/202206261343264.png)

   > 对于order M，有N个元素的B+树而言：
   >
   > $Depth = O(\lceil log_{\frac{M}{2}}N\rceil)\\ T_{find} = O(logN)$

   插入和删除操作：比较简单，可以参考[B+树插入、删除图文详解](https://www.cnblogs.com/nullzx/p/8729425.html)



### 1.2 Inverted File Index

倒排文件索引

定义：

* index：用于定位文章中term位置的途径
* inverted file：包含一系列指针（比如term出现的页数）

过程：

* 从文件中读取词
* 将该词提取为词干（word stemming），即除去第三人称形式、过去式、进行式等形式，留下词干，并除去stop word，即没有意义的词
* 检查该词是否已经在字典中。若不在，则将该词添加到词典之中。更新索引信息（包括词语、词语出现的总次数、文件号、该文件中该词位置）
* 建立完毕后，将索引文件存入磁盘

> while accessing a term by hashing in an inverted file index, range searches are expensive，但是hash搜索（O（1））要比二叉树搜索快O（logn）

索引文件存储方式：

* distributed indexing

  当倒排索引⽂件较⼤时，⽆法存在⼀台主机上，那么就涉及到倒排索引分布式存储技术。可以分为Termpartitioned Index与Document-partitioned Index。前者按照关键词将⽂件存在不同的主机上，全局建⽴索引，后 者按照⽂件号将⽂件存在不同主机上，在每台主机上建⽴局部索引。前者查找性能更⾼，⽽后者更加可靠。

  ![截屏2022-05-29 下午6.28.45](https://whm-image.oss-cn-beijing.aliyuncs.com/img/202206261343994.png)

* dynamic indexing

  想象⼀下平时搜索引擎的使⽤场景，索引是动态变化的。所以索引需要定时更新，并建⽴主索引和辅助索引。由于需要被索引的⽂档集可能是动态变化的（例如添加新⽂档、删除现有⽂档），因此索引需要适应这种变化。 最简单的更新办法是周期性地对⽂档集从头开始进⾏索引重构。如果要求能够及时检索到新⽂档，那么⼀种⽅法是 同时保持两个索引：⼀个是⼤的主索引，保存在磁盘中，另⼀个是⼩的⽤于存储新⽂档信息的辅助索引，辅助索引保存在内存中。检索时可以同时遍历两个索引并将结果合并。⽽⽂档的删除记录在⼀个⽆效位向量中，在返回检索 结果之前可以利⽤它过滤掉已经删除的⽂档。每当辅助索引变得很⼤时，就将它合并到主索引中。



* 压缩 compressing

  ⼀般来说，对索引⽂件进⾏压缩不但可以减⼩空间，并且可以提⾼索引效率。这是因为，采⽤⾼效的压缩算法，虽 然将耗费⼀定时间在内存中进⾏解压，但因为能提⾼cache的利⽤率，并能提⾼从磁盘到内存的读取效率，所以总 体来说效率将得到提升。 

  索引⽂件压缩的内容在课件中提到了两种实现⽅式：⼀是将词典看为单⼀字符串，以消除⽤定⻓⽅法来存储单词所存在的空间浪费；⼆是docID的存储只记录与上⼀项docID的差值来减少docID存储⻓度。



* 阈值 thresholding

  这部分内容是说在现实使⽤中⼈们往往只关⼼结果中的前⼀部分，所以搜索时可以通过只搜索前x%来提⾼效率。排序方式：

  * document：只检索前面x个按权重排序的文档
  * query：把待查询的terms按照出现的频率进行升序排序



* 搜索引擎的评价标准
  * how fast does it index 索引有多快
  * how fast does it search 搜索有多快
  * expressiveness of query language 查询语言的表现



* Relevance measurement：precision(精确度)、recall(召回率)

  ![截屏2022-05-29 下午6.40.47](https://whm-image.oss-cn-beijing.aliyuncs.com/img/202206261344820.png)



### 1.3 Heap

> Recall that the worst-case time complexities of insertions and deletions in a heap of size N are both O(logN). Then, without changing the data structure, the amortized time complexity of insertions in a heap is also O(logN), and that of deletions is O(1). 

1. Leftist Heap

   目标：加速两个堆的合并算法至O(logN)

   定义：

   * NPLc（NULL path length）：一个节点到达只有一个孩子的节点或者叶节点的后代的最短路径长度，将NULL定义为-1 $NPL(x) = min \{ NPL(c)\ c\ is\ a\ child\ of\ X\} +1$
   * leftist heap property：所有节点的左孩子的NPI不小于右孩子的NPI

   性质：

   * **⼀个右路径有r个节点的左倾树⾄少有$2^r-1$个节点**
   * 有N个节点的左倾树的右路径⾄多有$log(N+1)$个节点

   操作：

   * 合并：

     * 递归方法：每次合并需要比较两个左倾堆的根节点的大小，将大的合并在小的上面

     ```c++
     struct TreeNode
     {
     	ElementType Element;
     	PriorityQueue Left;
     	PriorityQueue Right;
     	int Npl;
     };
     
     
     PriorityQueue Merge(PriorityQueue H1, PriorityQueue H2)
     {
     	if(H1 == NULL) return H2;
     	if(H2 == NULL) return H1;
     	if(H1 -> Element < H2 -> Element)
     		return Merge1(H1, H2);
     	else
     		return Merge1(H2, H1);
     }
     
     PriorityQueue Merge1(PriorityQueue H1, PriorityQueue H2)
     {
     	if(H1 -> Left == NULL) //  对于左倾堆，左子树为空，则右子树一定也为空，即H1为单个节点
     		H1 -> Left = H2;
     
     	else{
     		H1 -> Right = Merge(H1 -> Right, H2);
     		if(H1 -> Left -> Npl < H1 -> Right -> Npl)
     			SwapChildren(H1);
     		H1 -> Npl = H1 -> Right -> Npl + 1;
     	}
     	return H1;
     }
     ```

     * 迭代方法

       * 将两个堆各自拆分成若干条right paths（只包含左子树和本身节点），将它们从小到大合并成一个

       * 自下而上检查子树是否满足左倾堆性质，不满足则交换左右节点

   * 删除：实质就是将根节点的两个子节点合并

   * 插入：原左偏树与一个只有一个节点的树合并

   > A perfectly balanced tree forms if keys 1 to $2^k - 1$ are inserted in order into an initally empty leftist heap
   >

2. Skew Heaps

   目标：斜堆是左倾堆的一个变种，它使得对于斜堆的连续M次操作最多消耗O(MlogN)的时间。从结构上来说，所有的左偏树都是斜堆，但反之不然。

   特点：

   * 没有Npl距离的概念
   * 每一次合并后都需要交换左右树

   合并方法：

   * 递归：左偏树一样，每次合并需要比较两个左倾堆的根节点的大小，将大的合并在小的上面
   * 非递归：同样是将两个堆各自拆分成若干条right paths（只包含左子树和本身节点），将它们从小到大合并成一个，只不过每次合并都交换左右子树

   > - 仅有一个节点的树为斜堆；
   > - 两个斜堆合并的结果仍为斜堆。
   > - **对于斜堆而言，插入、合并、删除的最坏时间复杂度都是O(N)，摊还代价都是O(logN)**
   > - 
   >   Comparing to leftist heaps, skew heaps are always more efficient in space

3. Binomial Queue

   目标：左倾堆将插⼊、合并和删除最⼩元的操作控制在O(logN)，尽管时间已经够少了，但⼆项队列进⼀步降低了这 个时间。⼆项队列（binomial tree）以最坏时间O（logN）⽀持以上操作，并且**插⼊操作平均花费常数时间**。

   特点：二项队列不是一棵树，而是树的集合，称为森林，这里的树有特定的形式，同时也具有堆序性，叫做二项树（binomial tree）。如图为一个二项队列，有B0,B1,B2,B3,B4五棵二项树。可以看到，二项队列具有以下特性：

   * 高度为k的二项树Bk通过将高度为Bk-1的二项树接到另外一棵高度为Bk-1的二项树的根上构成。
   * 二项树Bk由一个带有儿子B0，B1·····Bk-1的根组成（形状上），从Bk的构成过程可以知道这个特性，在构成B3的过程中，经历了根节点接上B0，B1，B2的过程。
   * 高度为k的树有2^k个节点，第k棵树的节点会等于第k-1棵树的节点*2
   * 对于每个Bk，深度d处的节点数为二项系数$C_K^d$

   ```c++
   typedef struct BinNode *position;
   typedef struct Collection *BinQueue;
   typedef struct BinNode *BinTree;
   
   struct BinNode
   {
   	int Element;
   	Position LeftChild;
   	Position NextSibling;
   };
   
   struct Collection
   {
   	int CurrentSize;
   	BinTree TheTrees[ MaxTrees ];
   };
   ```

   ![截屏2022-05-29 下午7.23.32](https://whm-image.oss-cn-beijing.aliyuncs.com/img/202206261344206.png)

   操作：

   * FindMin: 遍历每棵子树的根节点找到最小值即可，时间复杂度是O(logN)，或者也可以维护一个变量来存储最小值，如果这样则该操作只需要常数时间

   * Merge：将两个高度相同的树合并成新的二项树，取小的作为根节点，如果有三棵树则随机选取两棵树进行合并，时间复杂度是O(logN)

     ```c++
     BinTree combineTrees(BinTree T1, BinTree T2)
     {
     	if(T1 -> Element > T2 -> Element)
     		return combineTrees(T2, T1);
     	T2 -> NextSibling = T1 -> LeftChild;
     	T1 -> LeftChild = t2;
     	return T1;
     }
     
     BinQueue Merge(BinQueue H1, BinQueue H2)
     {
     	BinTree T1, T2, Carry = NULL;
     	int i, j;
     	if(H1 -> CurrentSize + H2 -> CurrentSize > Capacity)
     		ErrorMessage();
     	H1 -> CurrentSize += H2 -> CurrentSize;
     	for(i = 0, j = 1; j <= i -> CurrentSize; i++, j *= 2){
     		T1 = H1 -> TheTrees[i];
     		T2 = H2 -> TheTrees[i];
     		switch(4*!!Carry + 2*!!T2 + !!T1){
     			case 0:
     			case 1: break;
     			case 2: 
     				H1 -> TheTrees[i] = T2; 
     				H2 -> TheTrees[i] = NULL; 
     				break;
     			case 3:
     				Carry = combineTrees(T1, T2);
     				H1 -> TheTrees[i] = H2 -> TheTrees[i] = NULL;
     				break;
     			case 4:
     				H1 -> TheTrees[i] = Carry;
     				Carry = NULL:
     				break;
     			case 5:
     				Carry = combineTrees(T1, Carry);
     				H1 -> TheTrees[i] = NULL;
     				break;
     			case 6:
     				Carry = combineTrees(T2, Carry);
     				H2 -> TheTrees[i] = NULL;
     				break;
     			case 7:
     				H1 -> TheTrees[i] = Carry;
     				Carry = combineTrees(T1, T2);
     				H2 -> TheTrees[i] = NULL;
     				break;
     		}
     	}
     	return H1;
     }
     ```

   * Insert: 可以理解为一种特殊的Merge，时间复杂度是O（N），从一个空的二项队列开始插入N个元素最多消耗O（N）的时间，因此均摊到每一个操作上的时间复杂度是常数时间

   * Delete Min操作：找到最小的根节点将其删除，将这颗二项树剩余的节点移除作为一个新的二项队列，将剩下的若干二项树作为一个新的二项队列，一共得到两个二项队列，将两个二项队列合并。时间复杂度是O(logN)

     ```c++
     ElementType DeleteMin(BinQueue H)
     {
     	BinQueue DeletedQueue;
     	Position DeletedTree, OldRoot;
     	ElementType MinItem = Infinity;
     	int i, j, MinTree;
     
     	if(IsEmpty(H)){
     		PrihtErrorMessage();
     		return -Infinity;
     	}
     
     	// 找到最小值及其所在的二叉树
     	for(i = 0; i < MaxTrees; i++){
     		if(H -> TheTrees[i] && H -> TheTrees[i] -> Element < MinItem)
     		{
     			MinItem = H -> TheTrees[i] -> Element;
     			MinTree = i;
     		}
     	}
     	DeletedTree = H -> TheTrees[MinTree];
     	H -> TheTrees[MinTree] = NULL;
     
     	// 删除最小值
     	OldRoot = DeletedTree;
     	DeletedTree = DeletedTree -> LeftChild;
     	free(OldRoot);
     
     	DeletedQueue = Initialize();
     	DeletedQueue -> CurrentSize = (1 << MinTree) - 1;
     	for(j = MinTree - 1; i >= 0; j--){
     		DeletedQueue -> TheTrees[j] = DeletedTree;
     		DeletedTree = DeletedTree -> NextSibling;
     		DeletedQueue -> TheTrees[j] -> NextSibling = NULL;
     	}
     	H -> CurrentSize -= DeletedQueue -> CurrentSize + 1;
     	H = Merge(H, DeletedQueue);
     	return MinItem;
     }
     ```

## 2. 算法

### 2.1 Backtracking

回溯算法思想：回溯是⼀种算法思想，主要是通过递归来构建并求解问题，当发现不满⾜问题的条件时，就回溯寻 找其他的满⾜条件的解。回溯法的基本思路：考虑所有可能的情况进行注意验证，在验证的过程中进行合理的剪枝(pruning)

常⽤解决问题的思路：

1. choices 即在这个问题中，我们可以做哪些事情。例如在数独空格中我们的选择有0 - 9是个数字。
2. constraint 即我们并不能随⼼所欲的选择，在做选择的同时，有⼀定的约束会限制我们的选择。⽐如数独空格 中我们需要依据数独的规则，每个空格选填⼀个数字，并不是每个空格随⼼所欲的填0-9 任意⼀个都可以。
3. goals 或者我们叫做递归的停⽌条件。 即随着选择的进⾏，我们什么时候就不⽤再做选择了，即问题求解完成

对backtracking的理解完全可以参照DFS，可以进⾏参照的代码模版：

```c++
bool backtracking(int i)
{
	bool Found = false;
	if(i > N)
		return true; 
	for(each xi in si){
		OK = check((x1, ..., xi), R);
		if(OK){
			count xi in;
			Found = backtracking(i + 1);
			if(!Found)
				Undo(i);
		}
		if(Found) break;
	}

	return Found;

}
```

> In backtracking, if different solution spaces have different sizes, start testing from the partial solution with the smallest space size would have a better chance to reduce the time cost.应该选择从少到多的回溯方式，这样在剪枝的情况下可以排除更多的情况

例子：

* Eight Queens 八皇后问题

* Turnpike Reconstruction Problem 高速公路重构问题

  思路：

  * 由距离段数求出加油站个数
  * 首先确定起始位置 x0 = 0， 以及最大位置 xn = max
  * 从长到短试树

  ```c++
  bool Reconstruct(DistType X[], DistSet D, int N, int left, int right)
  // left和right表示还未找到位置的x的标号
  {
  	bool Found = false;
  	if(Is_Empty(D))
  		return true;
  	D_Max = Find_Max(D);
  
  	// 第一种尝试：x[right] = D_Max
  	OK = Check(D_Max, N, left, right); 
  	// 检查一下新加入的点是否满足条件（也就是对应的distance是否都在D之中）
  	if(OK){
  		X[right] = D_Max;
  		for(i = 1; i < left; i++) Delete(X[right] - X[i], D);
  		for(i = right++; i <= N; i++) Delete(X[i] - X[right], D);
  		// 从D中删去新加入的点产生的dist
  		
  		Found = Reconstruct(X, D, N, left, right - 1);
  
  		if(!Found){ // 如果不能实现，则撤回此次操作
  			for(i = 1; i < left; i++) Insert(X[right] - X[i], D);
  			for(i = right++; i <= N; i++) Insert(X[i] - X[right], D);
  		}
  	}
  
  	if(!Found){ // 第二种尝试：X[left] = X[N] - D_Max
  		OK = OK = Check(X[N] - D_Max, N, left, right);
  		if(OK){
  			X[left] = X[N] - D_Max;
  			for(i = 1; i < left; i++) Delete(X[left] - X[i], D);
  			for(i = right + 1; i <= N; i++) Delete(X[i] - X[left], D);
  
  			Found = Reconstruct(X, D, N, left + 1, right);
  
  			if(!Found){
  				for(i = 1; i < left; i++) Insert(X[left] - X[i], D);
  				for(i = right + 1; i <= N; i++) Insert(X[i] - X[left], D);
  			}
  		}
  	}
  	return Found;
  }
  ```

  

* Tic-tac-toe AI下棋问题

  * Minimax Strategy 最大最小策略

    ⽤⼀个评估函数来量化⼀个选择的好坏。在本例子中用goodness函数来衡量$f(P)=W_{AI}-W_{Human}$,W是当前情况下某一方可能赢的所有结果

  * Pruning 剪枝

    每次在max里向下取最大的，在min里向下取最小的，并且上面的值是由下一层的取出来得 到的，对于不会影响上一层取值的点就可以进行剪枝

    ![截屏2022-05-31 上午10.48.14](https://whm-image.oss-cn-beijing.aliyuncs.com/img/202206261344951.png)

    $\alpha - \beta \ purning$能将要查找的节点数减少到$O(N^{\frac{1}{2}})$

    




### 2.2 Divide and Conquer

分治法的基本概念：把⼀个复杂的问题分成两个或更多的相同或相似的⼦问题，再把⼦问题分成更⼩的⼦问 题……直到最后⼦问题可以简单的直接求解，原问题的解即⼦问题的解的合并。



基本思想和策略：

* 分治法的设计思想是：将⼀个难以直接解决的⼤问题，分割成⼀些规模较⼩的相同问题，以便各个击 破，分⽽治之。
*  分治策略是：对于⼀个规模为n的问题，若该问题可以容易地解决（⽐如说规模n较⼩）则直接解决，否 则将其分解为k个规模较⼩的⼦问题，这些⼦问题互相独⽴且与原问题形式相同，递归地解这些⼦问题， 然后将各⼦问题的解合并得到原问题的解。这种算法设计策略叫做分治法。

分治法的适⽤情况：

* 该问题的规模缩⼩到⼀定的程度就可以容易地解决 
* 该问题可以分解为若⼲个规模较⼩的相同问题，即该问题具有最优⼦结构性质。 
* 利⽤该问题分解出的⼦问题的解可以合并为该问题的解； 
* 该问题所分解出的各个⼦问题是相互独⽴的，即⼦问题之间不包含公共的⼦⼦问题。



例子：Closest Point Problem

* 暴力解法是$\frac{N(N-1)}{2}$求出最短距离
* 分治法解决思路：将问题分成三个子问题，对于每个点，分别从同侧的左右两边和异侧来找到距离他最近的点 $T(N) = 3T(N/3) + f(N)$，时间复杂度是O(NlogN)

求解分治法时间复杂度的方法：

T(N) = aT(N/b) + f(N)，使用下列方法的前提：当n足够小时，T(N)是常数时间

* Substitution method —— guess, then prove by induction

* Recursion-tree method

  * 递归树是平衡的，则可以直接计算结果
  * 递归树不平衡，可以推测出⼀个guess值，再⽤substitution method计算

* Master method

  * 第⼀类（⽐较复杂）并不能处理所有递归式

    ![截屏2022-05-30 上午9.28.15](https://whm-image.oss-cn-beijing.aliyuncs.com/img/202206261344100.png)

    * 第⼆类（较为简单） 同样不能处理所有递归式 但是可以处理的递归式与第⼀类并不完全相同

      ![截屏2022-05-30 上午9.28.37](https://whm-image.oss-cn-beijing.aliyuncs.com/img/202206261344078.png)

    * 第三类 专⻔处理某⼀类型的递归式（最常用）

      ![截屏2022-05-30 上午9.29.06](https://whm-image.oss-cn-beijing.aliyuncs.com/img/202206261344547.png)

### 2.3 Dynamic Programming

思想：solve sub-problems just once and save the answer in a table 用存储空间记录子问题的结果， 避免重复的运算

常见解决思路（以0-1背包为例）：

![截屏2022-05-30 下午9.37.39](/Users/weihaoming/Library/Application Support/typora-user-images/截屏2022-05-30 下午9.37.39.png)

例子：

1. Ordering Matrix Multiplications 矩阵乘法次数优化

   问题描述：给定⼀连串的矩阵乘法 $M_1 * M_2 ...M_n$，调整乘法进行的顺序，让总运算次数最⼩。注意$M_{ab} * M_{bc}$的次数是$a*b*c$

   解决思路：

   * f(i, j)表示从Mi乘到Mj的所有乘法中运算次数的最小值
   * 状态计算：$f(i,j) = min(f(i,l) + f(l,j)+i * l * j, i <l <j)$，而且f(i, i) = 0，也就是根据最后一次乘法的位置划分状态
   * 时间复杂度：$n^2$个状态，n个状态转移方程，时间复杂度是$O(N^3)$

2. Optimal Binary Search Tree 最优二叉搜索树

   问题描述：给定N个词，用二叉搜索树存储，以pi的概率去访问wi，则总的时间开销是$T(N)= \sum^N_{i=1}p_i(1+d_i)$di为wi所在的高度，让T（N）最小

   解决思路：

   * f(i, j) 表示从i到j的词用二叉搜索树存储最小的时间开销
   * 状态计算: $f(i, j)=min(f(i,l-1)+f(l+1,j)+\sum_i^jp_k, i <l<j)$，而且f(i, i) = pi，也就是根据二叉搜索树的根节点进行状态划分
   * 时间复杂度：$n^2$个状态，n个状态转移方程，时间复杂度是$O(N^3)$

3. All-Pairs Shortest Path 任意两点的最短路径

   问题描述：对于一个图，求出任意两点之间的最短路径长度。注意解决单源最短路问题我们通常使用Dijkstra算法，我们可以对每一个点遍历一遍该算法，时间复杂度是$O(V^3)$，在稀疏图的情况下更快

   解决思路：

   * f(i, j)表示从i到j最短路径的长度，f(i, i) = 0, f(i, j) = v(i, j)(当ij之间有通路时)
   * 状态计算: $f(i, j) = min(f(i,k) + v(k,j), k为任意一个与j节点相连的节点)$，根据到达j节点前的最后一个节点进行划分
   * 时间复杂度：$n^2$个状态，n个状态转移方程，时间复杂度是$O(N^3)$

4. Product Assembly 生产线问题

   问题描述：两条⽣产线，每⼀时间点可以选择两条⽣产线中的任⼀条，求最⼩的⽣产时间。

### 2.4 Greedy

思想：Make the best decision at each stage, under some greedy criterion. A decision made in one stage is not changed in a later stage, so each decision should assure feasibility.(找 到当前情况下的最优选择，然后进⼊下⼀情况，重复该过程，直到问题结束)

使用场景：

* 局部最优等于全局最优
* 求解全局最优的时间太长

使用策略：

* 看问题能否转化成做出⼀个选择之后，获得⼀个⼦问题
* 看最优解是否可由贪⼼选择(greedy choice)得来
* 证明问题存在最优⼦结构(optimal substructure)

例子：

1. Activity Selection Problem 活动安排问题

   动态规划解法：O(N^2)

   贪心：**选择尽早结束的活动 / 选择最迟开始的活动**

2. Huffman Code哈夫曼编码

   把出现频率高的用短的01串来编码，以达到压缩路径的目的，需要保证没有一个字符是另一个字符的前缀，否则存在多种解码方式

## 3. CS Theory

### 3.1 Amortized Analysis

特性：最坏bound >= 摊还bound >= 平均bound

> * 注意不是cost，而是bound，所以摊还cost一定小于平均cost这样的表述是错误的
> * 摊还分析不涉及概率上的分析，而是可以保证最坏情况下每个操作的平均性能

方法（以multiset为例）：

* Aggregate analysis 聚合分析

  * 基本思路：直接计算n个连续操作在最坏情况下需要消耗的总时间，然后再除以n，这种方法只能适用于比较简单的情况，大多数情况很难直接计算总时间

  * 本题解法：

    尽管一次multipop操作的最坏时间可能是O(N)的，但是pop的总次数不能超过push的次数，也就是最坏的n次连续操作（n-1次push，1次multipop）所花费的总时间仍然是O(N)的，因此除以n次操作，可以得知每一次操作的摊还时间是常数的

* Accounting method 核算法

  * 基本思路：对于不同的操作赋予不同的摊还时间，这个时间可能会不同于这一操作对应的实际时间，当一个操作的摊还代价大于它的实际代价时，我们可以将它存储起来，称为credit，用于支付后续摊还代价小于实际代价的操作，在这一过程中我们需要保证credit不能为负数，此时对于连续n次操作$\sum 摊还代价 = \sum实际代价+credit$，由于credit非负，我们就可以用$\sum 摊还代价$来表示连续n次操作所花费的最大时间

    > 这种方法与聚合分析相比，对每一个不同的操作都分别赋予不同的摊还时间，但是难点在于如何确定某一操作对应的摊还代价

  * 本题解法

    | 操作     | 实际代价 | 摊还代价 |
    | -------- | -------- | -------- |
    | push     | 1        | 2        |
    | pop      | 1        | 0        |
    | multipop | min(k,s) | 0        |

    我们设计摊还代价如上表，因为push的总次数永远大于pop的总次数，因此credit可以保证永远非福，则此时$\sum摊还代价$是O（N）的，因此每次操作的摊还时间是常数

* Potential method 势能法

  * 基本思路：对核算法进行一个改进，对credit进行一个定量的分析，我们可以定义一个势能函数$c_i^, = c_i + \theta(D_i)+\theta(D_{i-1})$,也就是我们需要定义一个将数据结构映射到某一数字的势能函数，对于每一个操作，它的摊还代价就是实际代价加上势能函数的变化值，由于我们仍然想要保证$\sum 摊还代价 = \sum实际代价$，也就是说，我们需要保证$\theta(D_n)-\theta(D_0)$大于0即可，因此我们确定势能函数需要保证最终值不得小于初始值，所以说一个好的势能函数需要让势能函数的初始值是最小值

    > 这种方法的难点就在于如何找到一个合适的势能函数

  * 本题解法

    我们选取$\theta(D_i)$表示第i次操作后stack中元素的个数，因此对于push操作 1+1 = 2， pop操作：1-1 = 0，对于multipop操作：k - k = 0，因此，再求得$\sum 摊还代价$为O（N），因此每次操作的摊还时间是常数



### 3.2 NP Completeness

1. 几个重要问题：

   * Euler circuit problem 欧拉回路问题: find a path that touches every edge exactly once 找到一条路径经过每条边恰好一次，一笔画问题，这是一个P问题，我们可以在多项式的时间内找到答案

   * Hamilton cycle problem: find a single cycle that contains every vertex 找到一个回路经过 每个点一次，我们没有找到多项式时间复杂度的算法，这是一个NP问题，我们很容易在多项式时间内验证一个解

   * Halting problem 停机问题: Is it possible to have your C compliler detect all infinite loops?---No 编译器不能发现所有的无限循环，这是一个不能被解决的问题（undecidable problems）

2. Turing Machine 图灵机

   * 模拟⼀个有⽆限时间、精⼒、纸笔的数学家
   * 组成部分：Infinite Memory(⽆限⻓的⼀维磁带)、Scanner
   * 确定性图灵机(deterministic turing machine)：A deterministic turing machine executes one instruction at each point in time. Then depending on the instruction and it goes to the next unique instruction 对于给定的输入每一步的执行都是唯一的图灵机
   * ⾮确定性图灵机(nondeterministic turing machine) ：A nondeterministic turing machine is free to choose its nect step from a finite set and is one of the steps leads to a solution, it will always choose the correct one 对于给定的输入可以自由选择执行的下一步，并且会选择正确的solution

   > 我们常说的NP问题，其实就是nondeterministic Polynomially

3. 区分P问题、NP问题、NPC问题和NP-hard问题：

   * P问题：有多项式时间算法，也就是算得很快的问题

   * NP问题：算起来不确定快不快的问题，但是我们可以快速验证这个问题的解

     NP问题是能在多项式时间内验证得出一个正确解的问题，也就是说，P类问题是NP类问题的一个子集（即存在多项式时间的算法，总能在多项式时间内验证它），但是P类问题和NP类问题是否完全等价仍在研究中

     > * Not all the decidable problems are in NP. 可描述的问题不全是NP问题
     > * 并⾮所有的可判断解是否正确的问题都是NP问题

   * NPC问题：属于NP问题，且属于NP-hard问题

     存在这样一个问题，所有的NP问题都可以约化成它，也就是说，只要解决了这个问题，所有的NP问题就都被解决了，其定义要满足两个条件：它是一个NP问题；所有的NP问题都能约化到它。通常证明一个问题是NPC问题的思路是：先证明它是一个NP问题，然后再证明其中一个NPC问题能约化到它

     > * 第一个被证明为是NPC问题的是Circuit Satisfiability问题(Circuit SAT)
     > * SAT问题，顶点覆盖问题，哈密顿回路问题、旅行商人问题(简称TSP问题)都是NPC问题，停机问题不是NPC问题

   * NP-hard问题：比NP问题都要难的问题

     NP-hard问题满足NPC问题的第二条定义但是不一定满足第一条定义（也就是说NP-hard问题没有限定该问题是一个NP问题，NP-hard问题要比NPC问题的范围更广），即所有的NP问题都可以约化到它，但是它不一定是一个NP问题



### 3.3 Approximation algorithm

1. 近似率

   对于一些非常hard的问题（比如NPC问题），要找到准确的答案需要的代价是很高的，我们可以退而求其次求一个近似值（常见的问题比如一些最大值最小值问题），衡量我们近似程度的参数是approximation ratio 近似率（对于任何规模为n的输入，C为算法的cost，C*为优化后的算法的cost，则$max(\frac{C}{c'}, \frac{C'}{C}) <= \rho(n)$，

   如果一个算法的近似率达到了$\rho(n)$,  则该算法可以被称为一个$\rho(n)-approximation\ algorithm$

2. 近似模式

   approximation scheme: 一个近似算法的时间复杂度不仅受n的影响，还受一个参数$\epsilon$的影响，对于给定的$\epsilon$，这个模式是一个$1+\epsilon$近似比的算法

   **PTAS（polynomial-time approximation scheme）：对于特定的$\epsilon$，近似模式的时间复杂度是一个关于n的多项式**

   **FPTAS（fully polynomial-time approximation scheme）：近似模式的时间复杂度是关于n或者关于$\epsilon$都是多项式的复杂度**

3. 例子

   * Bin Packing

     问题描述：给定n个物品（体积小于1），最少需要多少个体积为1的箱子来装（这是一个NPC问题）

     on-line 解法：在知道下一个物品大小之前就放好了此次物品，且不会更改，经过证明，Online的算法所需要的箱子的个数 不会少于最优解的5/3倍

     * **next fit**：判断最后一个箱子能否装下当前的物品，若能则装进去，不能则装进下一个空箱子。该算法是线性的，它的近似比为2
     * **first fit**：找到第一个能放下的箱子，该算法的时间复杂度是O(NlogN)，近似比约为1.7
     * **best fit**：在所有能放下当前物品的箱子中，找剩余空间最小的箱子，该算法的时间复杂度是O(NlogN)，近似比约为1.7

     off-line算法：在装箱之前对所有箱子从大到小排序，然后再用first fit或者best fit算法装箱子，该算法的时间复杂度是O(NlogN)，**近似比为11/9**

     经验：简单的贪心方法会给出一些比较好的近似效果

   * Knapsack Problem 0-1背包问题

     问题描述：将N个物品装入容量为M的背包里，每个物品有自己的重量Wi和收益Pi，最终目标是要让收 益最大化

     使用贪心算法的近似率是2

     使用动态规划的方法求解，复杂度是$O(n^2p_{max})$

     **背包问题是 NP-hard 问题，但是如果对于规模为n的背包问题，没有一个物品的大小超过多项式复 杂度则不是NP-hard问题**

     > Consider a Knapsack problem with *n* items. If no items have a size larger than *n*3, then it is no longer NP-hard.  **True**

   * K-center problem

     问题描述: 在给定的N个点的平面中选择K个点(不一定是已有的点)作为中心作圆覆盖所有的点，要 求使得这些圆中的最大半径(distance)取得最小值
   
     **一种贪心的思路: 将第一个中心放在尽可能好的地方，然后不断加入点使得覆盖半径减小,这种算法的近似率是2，因此是个2-approximation,除非P=NP，否则K-center问题不存在近似率小于2的逼近算法**
   
   > A randomized algorithm for a decision problem with one-sided-error and correctness probability 1/3 (that is, if the answer is YES, it will always output YES, while if the answer is NO, it will output NO with probability 1/3) can always be amplified（放大） to a correctness probability of 99%.



### 3.4 Local Search

1. 算法思想：

   局部搜索算法是一种启发式的算法，简单来说，该算法每次从当前解的邻域空间中选择一个最好的邻居作为下次迭代的当前解，直到达到一个局部最优解（local optimal solution）。这种算法思想很单纯，但是也存在一个很大的缺陷，在搜索选择的过程中可能会陷入局部最优解，而并非全局最优解

2. 局部搜索的框架：

   * local：a local optimum is a best solution in a neighborhood
     * 邻居关系：S ~ S’: S‘ is a neighboring solution of S – S' can be obtained by a small modification of S.
     * N(S): neighborhood of S
   * Search：在neighborhood中从一个点出发，找到局部最优的解，gradient descent —— 梯度下降法，找梯度下降最快的方向来优化

   ```c++
   SolutionType gradient_descent()
   {
   	start from a feasible solution S in FS;
   	MinCost = cost(S);
   	while(1)
   	{
   		S‘ = Search(N(S)); // find the best s' in N(S)
   		CurrentCost = cost(S‘);
   		if(CurrentCost < MinCost)
   		{
   			MinCost = CurrentCost;
   			S = S‘;
   		}
   		else
   			break;
   	}
   	return S;
   }
   ```

3. 例子

* Vertex Cover Problem 顶点覆盖问题

  问题描述：在一个无向图中找到一个最小点集，使得对于G的每一条边，都至少有一个顶点在S中

  方法：

  * 快速找到一个feasible solution，本题中我们选择all vertex 作为一开始的S
  * 然后需要根据问题定义一个cost函数：本题中显然我们选择|S|作为cost(S)
  * 我们还需要定义邻居关系：本题中我们将一个S增加或删除一个节点可以到达的节点集合作为S的邻居
  * local search的具体步骤；从S的邻居中任意选择一个；如果被选择的cost优于当前S，则更新S为该邻居；如果被选择的cost差于当前S，则以一定概率更新S为该邻居

  > 这一问题的难点在于如何计算更新的概率公式

* Hopfield Neural Network

  问题描述：给定一个带权重的无向图，每一个节点都有+1或-1两种状态，如果一条边的权重小于0，则该边的两个节点状态应该相同，若权重大于0，则两个节点的状态应该相反。我们的目标是找到一种**足够好**的对每一个节点状态法分配方式(configuration)

  > "足够好"是一个模糊的定义，用更规范化一点的表述方式应该为：
  >
  > 首先，对于每一个configuration，每一条边的good/bad是这样定义的：如果一条边的$w_es_us_v < 0$，则这条边是好边（和之前我们对状态的理想描述是一致的）
  >
  > 然后我们可以定义一个节点是否satisfied：如果一个节点的好边的权重之和大于坏边的权重之和，则称这个节点是satisfied的
  >
  > 最后我们可以给出一个configuration足够好（stable）的标准：如果一个configuration可以使所有的节点都是satisfied，那么我们就可以说这个configuration是stable的

  解法：State-flipping Algorithm

  如果还没有达到stable的configuration，则随机选择一个不是satisfied的点，翻转其状态，重复该步骤直到找到一个stable的configuration，经过证明，这种方法总能找到一个stable 的configuration，时间复杂度是伪多项式的

  从local search的角度理解该算法：

  * FS（feasible solution）：随机一个configuration
  * 定义cost函数：所有good的边的权重之和
  * 邻居关系：将S中任意一个节点的状态进行翻转得到的configuration的集合为S的邻居
  * local search：如果还没有达到stable的configuration，则随机选择一个不是satisfied的点，翻转其状态，重复该步骤直到找到一个stable的configuration

  该算法可以保证在$\sum|w_e|$次循环之前可以停止

* Max Cut Problem

  问题描述：在有权重（权重均为正数）的无向图中找到一种划分方式（可以理解为对图中所有节点进行黑白两色的染色），使得两个节点颜色不同的边的权重之和最大

  > 该问题其实和Hopfield Neural Network很类似，是之前问题在所有边权重都是正数时的一种特殊情况，所以解决思路也是类似的，不过这种方法我们只能得到一个local optimum solution，不能保证结果是global optimum的，不够这种方法可以保证$w(A,B) >= \frac{1}{2}w(A^*,b^*)$，分别表示局部最优解和全局最优解

  * S（feasible solution）：随机一种涂色方式
  * 定义cost函数：所有good的边的权重之和
  * 邻居关系：将S中任意一个节点的状态进行翻转得到的configuration的集合为S的邻居

  可能不能在多项式时间复杂度内结束算法，因此，我们考虑当local search对结果的优化效果 不明显时，就停⽌local search

  Big-improvement-flip

  * 当翻转⼀个点带来的优化效果⼩于$\frac{2\epsilon}{|V|}w(A,B)$时，就停止继续做local search
  * 这样有$(2+\epsilon)w(A,B) >= w(A^*,B^*)$
  * 最多进行$O(\frac{n}{\epsilon}logW)$次翻转

  最大分割问题存在近似于为1.1382的逼近算法，但是没有近似率低于17/16的

> The local search always return an optimal solution for Minimum Spanning Tree

### 3.5 Random Algorithm

1. 算法思想：

   The algorithm behaves randomly – make random decisions as the algorithm processes the worst-case input 算法运行过程中会做一些随机的决策

   两种随机算法：

   * efficient randomized algorithms that only need to yield the correct answer with high probability 运行效率高，但是不是总正确

   * randomized algorithms that are always correct and run efficiently 

     in expectation 正确性好，但是不是总高效

2. 几个重要概念

   * $pr[A]$：A事件发生的可能性
   * $\overline A$：A事件不发生
   * $pr[A] + pr[\overline A] = 1$
   * $E[X]$：X的数学期望

   > 如果用定义求E[X]的话需要知道所有情况下X的可能性，这种方法通常并不可行，需要我们换一种思路解决该问题

3. 例子

* hiring problem 雇佣问题

  问题描述：现在需要雇佣一个职员，N天内每天面试一个人，如果有比现有的好则用它来替换现有的，在这个过程中，interview的成本远远小于hiring的成本，总的成本为$O(NC_i+MC_h),C_i是面试成本, C_h是雇佣成本$

  朴素解法：

  ```c++
  int Hiring(EventType c[], int N)
  { // 第0号为一个dummy head
    int Best = 0;
    int BestQ = the quality of candidate 0;
    for(i = 1; i <= N; i++){
      Qi = interview(i);
      if(Qi > BestQ){
        BestQ = Qi;
        Best = Q;
        hire(i);
      }
    }
    return Best;
  }
  ```

  这种解法可以保证得到最优解，但是在worst case（面试者的quality是递增的情况下）成本会非常高，达到$O(NC_h)$，如果参加面试者的顺序是随机的，经过证明，总的cost是$O(C_hlnN+NC_i)$

  随机算法：

  ```c++
  int Hiring(EventType c[], int N)
  { // 第0号为一个dummy head
    int Best = 0;
    int BestQ = the quality of candidate 0;
    
    random permute the lists of candidate; // 花费一定时间，但好处是不需要假定面试者来面试顺序是随机的也可以保证面试者的quality是随机排列的
    
    for(i = 1; i <= N; i++){
      Qi = interview(i);
      if(Qi > BestQ){
        BestQ = Qi;
        Best = Q;
        hire(i);
      }
    }
    return Best;
  }
  
  void PermuteBySorting(ElemType A[], int N)
  {
  	for(i = 1; i <= N; i++)
      A[i].P = i + rand() % (N^3);
    	// 让每个数的优先级尽可能不一样
    	sort A, using P as the sort keys;
  }
  ```

  随机排序花费一定时间，但好处是不需要假定面试者来面试顺序是随机的也可以保证面试者的quality是随机排列的

  online 解法

  算法思想：先面试前K个人不录取，在后面的N-K个人中选出第一个比前面K个最高分要高的人

  ```c++
  int Online(EventType c[], int N, int k)
  {
    int Best = N;
    int BestQ = -INF;
    for(i = 1; i <= k; i++){
      Qi = interview(i);
      if(Qi > BestQ)
        BestQ = Qi;
    }
    
    for(i = k+1; i <= N; i++){
      Qi = interview(i);
      if(Qi > BestQ){
        Best = i;
        break;
      }
    }
    return Best;
  }
  ```

  这种解法不能保证一定会录取quality最高的候选人，但是由于只进行一次录取，cost很低，**通过数学方法我们可以证明被录取的人就是quality最高的人的概率是**$P(S) = \frac{k}{N}\sum_{i=k}^{N-1}\frac{1}{i}$

  通过求导，我们可以得出当$k=\frac{N}{e}$时这个概率是最高的

* quick sort 快速排序

  算法思想：在确定性的快速排序算法中，快速排序的最差时间复杂度是$O(N^2)$，平均时间复杂度是$O(NlogN)$，平均时间复杂度的前提是每一种输入的顺序出现的机会都是可能的，我们可以随机选择一个位置作为pivot来在算法中保证这个前提的成立

  * central splitter：将数组分成两段的pivot并且每段至少是总长度的1/4
  * Modified Quicksort：在开始递归之前选择出一个中心分割点
  * 最终随机选择pivot的快速排序的复杂度是$O(NlogN)$

### 3.6 Parallel Algorithm

1. 算法思想：

   之前我们讨论的算法都是串行运行的，也就是每一次完成一个operation，对于并行算法，它可以一次性完成多个operation，从而缩短了程序运行的时间，我们常见的并行算法模型有两种：

   * Parallel Random Access Machine(PRAM)

     PRAM模型下有多个precessor和一个共享的memory，对于每个precessor，它们对于memory的每一次read/write/computation的操作都要消耗一个时间单元（unit time access），这种算法可能会面临多个precessor同时访问一个memory单元的情况，为了解决这种冲突，PRAM有三种解决这种access conflicts的方法：

     * Exclusive-Read Exclusive-Write (EREW) 不能同时读写一个位置
     * Concurrent-Read Exclusive-Write (CREW) 可以同时读，不能同时写
     * Concurrent-Read Concurrent-Write (CRCW) 可以同时读写（为了实现同时读写，有三种实现方法：Arbitrary rule 每一次随机选择processor进行、Priority rule 最小标号的P优先使用、Common rule 如果所有要写的值一样就同时写

     例子：计算n个数的和

     ```c++
     for Pi, 1 <= i <= n pardo
       B(0,i) := A(i) // 将n个数字读入
       for h = 1 to logn do: // 对每一层进行加法运算 
     	if i <= n/2^h
         	B(h, i) := B(h-1, 2i-1) + B(h-1, 2i)
         else stay idle
      for i = 1: output B(log n, 1); for i > 1:stay idle // 将结果输出
     ```

      这个算法所花费的时间为logn + 2，但是它还有几个缺点：

     * 这个算法没有体现在processor数量不同的情况下，程序运行的效率会有何变化
     * Fully specifying the allocation of instructions to processors requires a level of detail which might be unnecessary  对每一个instruction都确定好地分配固定的processor，既浪费时间，也没必要

   * Work-Depth(WD)算法模型

     这种算法模型很好地规避了上面两个缺点，计算n个数的和：

     ```c++
     for Pi, 1 <= i <= n pardo
       	B(0, i) := A(i)
       for h = 1 to log n
         	for Pi, 1 <= i <= n/2^h pardo
             	B(h, i) := B(h-1, 2i-1) + B(h-1, 2i)
      for i = 1 pardo
        	output B(log n, 1)
     ```

2. Measuring the performance 衡量并行算法的规则

   两个概念：

   * Work load – total number of operations: W(n) 总的操作数量
   * Worst-case running time: T(n) 最坏运行时间

   4个渐进等价的式子：

   * W(n)次操作，花费T(n)时间
   * **PRAM：当processor个数P(n) = W(n) / T(n)时，花费T(n)时间**
   * **PRAM：当processor个数P(n) < W(n) / T(n)时，花费W(n)/p 时间**
   * **PRAM：对于任意数量的p，花费W(n)/P + T(n)的时间（当processor数量冗余）**

   以上面WD算法下的计算n个数字和为例：

   $W(n) = n+\frac{n}{2}+\frac{n}{2^2}+...+\frac{n}{2^k}+1,\ where\ 2^k = n$

   $W(n) = 2n$

   $T(n) = logn + 2$

   我们可以发现，相比于串行算法，总的操作数量是一样的，但是花费的时间有所减少

3. WD-presentation Sufficiency Theorem

   任何WD模型下的算法，用P(n)个processor，运行时间都不超过$O(\frac{W(n)}{P(n)}+T(n))$

4. 例子：

   * Prefix Sum 前缀和

     问题描述：给出n个数，计算n个前缀和

     算法思想：用平衡二叉树解决，先从下往上计算B的值，再从上往下计算C的值

     ![截屏2022-06-05 下午5.46.08](https://whm-image.oss-cn-beijing.aliyuncs.com/img/202206261344355.png)

     ```c++
     for Pi, 1 <= i <= n pardo
       	B(0, i) := A(i)
       
      // 从下往上计算B的值，和上一节的并行求和算法一样
     for h = 1 to log n
       	for i, 1 <= i <= n/2^h pardo
           B(h, i) := B(h-1, 2i-1) + B(h-1, 2i)
     
           
     // 从上往下计算C值，根据节点位置和B值求解
      for h = log n to 0
        	for i even, 1 <= i <= n/2^h pardo
           	C(h, i) = C(h + 1, i /2) // 如果一个节点是它父节点的右节点，则它的值和父节点值一样
         for i = 1 pardo
           	C(h, i) := B(h,1) // 对于最左边的路径上的节点，它的C值和B值相等
         for i odd, 3 <= i <= n/2^h pardo
           	C(h, i) := C(h+1, (i-1)/2) + B(h,i) // 对于不在最左路径的节点，如果它是父节点的左节点，则它的值等于它左边节点的父节点的值加上本身的B值
           
     for Pi, 1 <= i <= n pardo
       	output C(0, i) // 最后二叉树的叶节点存储着n个前缀和的值
       
     ```

     复杂度：

     T(N) = O(log N) W(N) = O(N)

   * Merging 数组合并

     问题描述：给定两个单调不减的数组，要求合并成一个单调不减的数组

     为了便于分析，我们将问题简化：两个数组大小相等、两个数组中元素各不相同、log n是正数

     算法思想：用partition的分割思想解决，先把输入分割成若干个子输入，然后再并行地分别解出子问题的答案 ，对于本问题，我们的思想核心是把合并问题转化成排序问题，如果知道A数组中每一个元素的位置，B数组每一个元素在A中的位置，就可以知道A、B数组中每一个元素在合并后数组C中的序号

     对于排序，我们引用了RANK的概念：

     * RANK(j, A) = i, if A(i) < B(j) < A(i+1), for 1 <= i <= m
     * RANK(j, A) = 0, if B(j) < A(1)
     * RANK(j, A) = n, if B(j) > A(n)

     如果我们知道了每一个元素的RANK序号，合并起来的并行算法T(n) = O(1)，W(n) = n + m

     并行合并算法：

     ```c++
     for Pi, 1 <= i <= n pardo
       	C(i + RANK(i, B)) := A(i)
     for Pi, 1 <= i <= n pardo
       	C(i + RANK(i, A)) := B(i)
     ```

     那么如何解决RANK问题，求解每一个元素的RANK号呢：

     * 方法一：二分查找

       ```c++
       for Pi, 1 <= i <= n pardo
         	RANK(i, B) := BS(A(i), B)
         	RANK(i, A) := BS(B(i), A)
       ```

       二分查找的方法虽然T(n) = O(log n)，但是W(n) = O(nlogn)， 工作量增加了

     * 方法二：线性排序

       ```c++
       i = j = 0
       while(i <= n || j <= m){
       	if(A(i+1) < B(j+1))
           	RANK(++i, B) = j
         else
           	RANK(++j, A) = i
       }
       ```

       这种线性排序的方法虽然使W(n) = O(n + m)，但是没有体现并行计算的优势，T(n) = O(n)时间上不能让人满意

     * 方法三：并行排序

       第一步：从n个元素中找出$p = \frac{n}{logn}$个select，每隔logn设置一个select，并计算它们的RANK，可以用二分查找确定RANK，这样T(n) = O(log n)， W(n) = O(plogn) = O(n)

       ![截屏2022-06-06 上午9.24.30](https://whm-image.oss-cn-beijing.aliyuncs.com/img/202206261344485.png)

       第二步：如图所示，在第一步完成后，会产生2p个箭头，可以分割成2p个part，而且每一个part之间是有序排列的，然后并行地对每一个part用线性排序，这样T(n) = O(part内元素个数) = O(logn)， W(n) = 2p * O(part内元素个数) = O(p logn) = O(n)

       综合以上两步，我们可以得知总的T(n) = O(log n)，W(n) = O(n)符合我们的预期

   * Maximum Finding 找最大值

     问题描述：很简单，在一个大小为n的数组中找到最大值，我们的目标是在O(1)的时间复杂度下完成目标

     一些尝试：

     * 二叉树的方式：用类似求数组和（例子1）的方式用二叉树的方式求解最大值，不过这种方法的T(n) = O(logn), W(n) = O(n)显然时间方面没有满足我们的要求
     * 对所有的数进行并行比较：并行地将所有数两两相比，虽然T(n) = O(1), 但是W(n) = O(n^2)显然不能尽如人意

     更进一步的思考：

     算法思想1: A Doubly-logarithmic Paradigm 顾名思义利用log log n求解，本题中我们设 h = log log n，假设h为整数

     * 朴素版本：

       * 将数组分成$\sqrt n$个小部分，并行地计算$\sqrt n$部分，分别求出最大值，求最大值大方法也是doubly-log，不过数组大小被缩小为$\sqrt n$, $T(n) = T(\sqrt n), W(n) = \sqrt  n\  T(\sqrt n)$
       * 在求出的$\sqrt n $个最大值中再求出最大值（用全比较大方式），T(n) = O(1), W(n) = O(n)

       综合以上两部操作，我们得出$T(n) <= T(\sqrt n) + O(1), W(n) <= \sqrt n W(\sqrt n) + O(n)$，最终解得$T = O(log log n), W = O(nloglogn)$，虽然和之前算法相比已经有了很大提升，不过还是不够

     * 优化版本：

       * 将数组划为长度为h大n/h部分，分别计算最大值（求最大值大方法就是简单遍历），则$T(n) = O(h), W(n) = \frac{n}{h}O(h)$
       * 在上一步找出的n/h个元素中找最大值（朴素版本的double-log算法），得出$T = O(log log \frac{n}{h}), W = O(\frac{n}{h}loglog\frac{n}{h})$

       综合以上两步操作，我们得出$T(n) = O(loglogn), W(n) = O(n)$已经非常好了，但是还是没有满足我们常数时间完成的目标

     

     算法思想2: Random Sampling很高地可能性下实现了T(n) = O(1), W(n) = O(n)

     * 首先随机从n个数中选取$n^\frac{7}{8}$个数，我们接下来要在这些数中求出最大值， $T = O(1), W = O(n^\frac{7}{8})$
     * 将这些数每一块大小为$n^\frac{1}{8}$，分成$n^\frac{3}{4}$个block，对于每一个block，我们采取全比较的方式求出最大值 $T = O(1), W = n^\frac{6}{8} * O((n^\frac{1}{4})^2) = O(n)$，此时我们获得了$n^\frac{6}{8}$个局部最大值
     * 将这些数每一块大小为$n^\frac{1}{4}$， 分成$n^\frac{1}{2}$个block，用类似的方法我们可以求出$n^\frac{1}{2}$个最大值，这一步骤$T = O(1), W = n^\frac{6}{8} * O((n^\frac{1}{4})^2) = O(n)$
     * 按照这种方法进行下去，我们接触最开始随机选择的$n^\frac{7}{8}$个数中的最大值， $T = O(1), W = O(n^\frac{7}{8})$

     找到之后我们可以再查看剩余未被选中的元素中是否有更大的，一旦有更大的，我们就重新进行Random Sampling，经过证明，我们得出这样的定理：Random Sampling很高地可能性（达不到该T、W的概率仅为$O(\frac{1}{n^c})$)下实现了T(n) = O(1), W(n) = O(n)

     

     

### 3.7 External Sorting

在内存中我们常常使用quicksort作为排序算法，但是对于一些大数据，内存放不下时，只能存储在disk中，此时由于磁盘IO的访问效率太低，快排不再适合，我们选择外部排序（一种基于至少3个tapes的归并排序）。tape的特点：只能顺序地访问，不能像数组一样直接寻址。

1. 算法思想：

   假设内存中可以存储M条记录，我们要对N个元素进行排序（一个排好序的子序列称为run）

   * 第一步每次从内存中读出M个数据放入内存中进行排序，将排序的结果放在另外的tape中
   * 在原来的tape和新的3个tapes中进行归并
   * 总的循环pass次数 = $1+\lceil log(N/M)\rceil$

2. 优化目标：

   * 减少pass的次数
     * 第一种思路是在原有算法的基础上增加tapes的个数，使2路合并变成k路合并，此时需要2k个tapes，消耗的pass数为$1+\lceil log_k(N/M)\rceil$，但是k也不是越大越好
     * 另一种思路比较巧妙，它可以用更少的tapes来完成k路归并，它不再对所有的run进行对半拆分，而是采用不对等拆分的方式来做。**k路的merge需要k+1个tapes，如果run的个数是斐波那契数列Fn的话就拆分称Fn-1， Fn-2，... Fn-k；如果不是斐波那契数列，可以增加一些新的run来凑成斐波那契数列**
   * 加速run merging
     * 使用哈夫曼树，每次把最短的两个run进行合并 T=O(the weighted external path length)
   * 并行操作的缓冲处理
     * 为了实现并行操作，我们将内存分为多个buffer，一部分为input buffer，另一部分为output buffer，对于一个k路的合并，**我们需要2k个输入buffer和2个输出buffer**。不过对于并行操作来说，并不是k越大越好，因为如果k增大，就会导致buffer size减少，导致磁盘中的block size减少，增加了IO访问次数，因此K的值应该取决于磁盘的参数和外存的规模
   * run generation 生成更长的run
     * **使用堆的结构来进行排序操作**，规则是一直取出堆中现存的可以放在现在所在的run后面的最 小的数，直到堆中的数据都放不进当前run了再更换一个run。如果内存可以容纳M个元素，则这种方法生成的run的平均长度为2M 再输入的元素接近已经排好序的状态时非常work





 
